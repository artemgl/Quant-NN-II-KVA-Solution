{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T15:23:36.661531Z",
     "start_time": "2024-11-14T15:23:29.814432Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/uvd174/miniconda3/envs/quant_env/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    XLMRobertaForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import wandb\n",
    "\n",
    "import pennylane as qml\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "os.environ['WANDB_PROJECT'] = 'quantum-peft-for-sentiment-analysis'\n",
    "os.environ['WANDB_ENTITY'] = 'uvd174'\n",
    "os.environ['WANDB_WATCH'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11ad37a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumLayer(nn.Module):\n",
    "    def __init__(self, n_qubits: int):\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        \n",
    "        self._dev = qml.device('default.qubit', wires=n_qubits)\n",
    "        self._weight_shapes = {'weights': (3, n_qubits, 3)}\n",
    "        \n",
    "        @qml.qnode(self._dev)\n",
    "        def qnode(inputs, weights):\n",
    "            qml.templates.AngleEmbedding(inputs, wires=range(self.n_qubits))\n",
    "            qml.templates.StronglyEntanglingLayers(weights, wires=range(self.n_qubits))\n",
    "            return [qml.expval(qml.PauliZ(wires=i)) for i in range(self.n_qubits)]\n",
    "        \n",
    "        self.qlayer = qml.qnn.TorchLayer(qnode, self._weight_shapes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.qlayer(x)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class QuantumLoraConfig:\n",
    "    r: int\n",
    "    lora_alpha: int\n",
    "    lora_dropout: float\n",
    "\n",
    "\n",
    "class QuantumLoraAdapter(nn.Module):\n",
    "    def __init__(self, layer: nn.Module, config: QuantumLoraConfig):\n",
    "        super().__init__()\n",
    "        assert isinstance(layer, nn.Linear), 'Layer must be an instance of nn.Linear'\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        self.lora_A = nn.Linear(layer.in_features, config.r, bias=False)\n",
    "        self.lora_B = nn.Linear(config.r, layer.out_features, bias=False)\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "        self.scaling = config.lora_alpha / config.r\n",
    "        self.dropout = nn.Dropout(p=config.lora_dropout)\n",
    "        \n",
    "        self.quantum_layer = QuantumLayer(config.r)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.lora_B(self.quantum_layer(self.lora_A(self.dropout(x)))) * self.scaling\n",
    "\n",
    "class QuantumLoraLinear(nn.Module):\n",
    "    def __init__(self, layer: nn.Linear, config: QuantumLoraConfig):\n",
    "        super().__init__()\n",
    "        self.base_layer = layer\n",
    "        self.lora = QuantumLoraAdapter(layer, config)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.base_layer(x) + self.lora(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d591c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_quantum_lora_to_xlm_roberta_model(\n",
    "        model: XLMRobertaForSequenceClassification,\n",
    "        lora_config: QuantumLoraConfig,\n",
    "):\n",
    "    for name, module in model.named_modules():\n",
    "        parent_name = '.'.join(name.split('.')[:-1])\n",
    "        module_name = name.split('.')[-1]\n",
    "        \n",
    "        if isinstance(module, nn.Linear) and ('query' in module_name or 'key' in module_name or 'value' in module_name):\n",
    "            parent = model\n",
    "            if parent_name:\n",
    "                parent = reduce(getattr, parent_name.split('.'), model)\n",
    "            \n",
    "            quantum_layer = QuantumLoraLinear(module, lora_config)\n",
    "            setattr(parent, module_name, quantum_layer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e52be6af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T15:23:36.692965Z",
     "start_time": "2024-11-14T15:23:36.664534Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>отзывы</th>\n",
       "      <th>разметка</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Оболочка после чистого андроида тоже очень нек...</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Нормальный телефон, очень красивая задняя панель</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Деньги на ветер .</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ну так себе</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ценник вполне адекватный для такой мощной нови...</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              отзывы разметка\n",
       "0  Оболочка после чистого андроида тоже очень нек...        -\n",
       "1   Нормальный телефон, очень красивая задняя панель        +\n",
       "2                                  Деньги на ветер .        -\n",
       "3                                        ну так себе        -\n",
       "4  Ценник вполне адекватный для такой мощной нови...        +"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the dataset from CSV file\n",
    "df = pd.read_csv('task-3-dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56b4832a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T15:23:36.723905Z",
     "start_time": "2024-11-14T15:23:36.695968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Distribution:\n",
      "разметка\n",
      "+    121\n",
      "-     89\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentages:\n",
      "разметка\n",
      "+    57.619048\n",
      "-    42.380952\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Display target distribution\n",
    "sentiment_counts = df['разметка'].value_counts()\n",
    "print(\"Target Distribution:\")\n",
    "print(sentiment_counts)\n",
    "print(\"\\nPercentages:\")\n",
    "print(sentiment_counts / len(df) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1cf9de8de9a5a57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T15:23:36.739070Z",
     "start_time": "2024-11-14T15:23:36.726912Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create dataset class\n",
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.encodings = tokenizer(texts, truncation=True)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf94904ef00d937",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T15:26:38.932130Z",
     "start_time": "2024-11-14T15:23:37.616427Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33muvd174\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/uvd174/Quant-NN-II-KVA-Solution/problem3/wandb/run-20241115_101216-4lozq3nu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/uvd174/quantum-peft-for-sentiment-analysis/runs/4lozq3nu' target=\"_blank\">xlm-roberta-large</a></strong> to <a href='https://wandb.ai/uvd174/quantum-peft-for-sentiment-analysis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/uvd174/quantum-peft-for-sentiment-analysis' target=\"_blank\">https://wandb.ai/uvd174/quantum-peft-for-sentiment-analysis</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/uvd174/quantum-peft-for-sentiment-analysis/runs/4lozq3nu' target=\"_blank\">https://wandb.ai/uvd174/quantum-peft-for-sentiment-analysis/runs/4lozq3nu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='295' max='910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [295/910 17:02 < 35:46, 0.29 it/s, Epoch 22.62/70]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.730300</td>\n",
       "      <td>0.704864</td>\n",
       "      <td>0.454545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.703100</td>\n",
       "      <td>0.694084</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.677300</td>\n",
       "      <td>0.701789</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.685900</td>\n",
       "      <td>0.696995</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.672700</td>\n",
       "      <td>0.704966</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.649300</td>\n",
       "      <td>0.718236</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.680900</td>\n",
       "      <td>0.704474</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.653300</td>\n",
       "      <td>0.702782</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.658400</td>\n",
       "      <td>0.725158</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.634900</td>\n",
       "      <td>0.701692</td>\n",
       "      <td>0.454545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.629600</td>\n",
       "      <td>0.712483</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.649200</td>\n",
       "      <td>0.698551</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.568100</td>\n",
       "      <td>0.661666</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.583700</td>\n",
       "      <td>0.652046</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.549900</td>\n",
       "      <td>0.600147</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.554700</td>\n",
       "      <td>0.539627</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.435600</td>\n",
       "      <td>0.506538</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.444800</td>\n",
       "      <td>0.483792</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.338700</td>\n",
       "      <td>0.423290</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.294400</td>\n",
       "      <td>0.416601</td>\n",
       "      <td>0.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.280300</td>\n",
       "      <td>0.410477</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.225800</td>\n",
       "      <td>0.292247</td>\n",
       "      <td>0.818182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load pretrained model and tokenizer\n",
    "model_name = 'FacebookAI/xlm-roberta-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Configure LoRA parameters\n",
    "lora_config = QuantumLoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "model = apply_quantum_lora_to_xlm_roberta_model(model, lora_config)\n",
    "\n",
    "# Freeze all layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "# Unfreeze lora adapters\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, QuantumLoraAdapter):\n",
    "        for param in module.parameters():\n",
    "            param.requires_grad = True\n",
    "# Unfreeze the classifier\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Prepare data\n",
    "texts = df['отзывы'].tolist()\n",
    "labels = (df['разметка'] == '+').astype(int).tolist()\n",
    "\n",
    "# Split data\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    texts, labels, test_size=0.05, random_state=42, stratify=labels,\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SentimentDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = SentimentDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "# Define metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\n",
    "        'accuracy': (predictions == labels).mean()\n",
    "    }\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='final-results',\n",
    "    num_train_epochs=70,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=256,\n",
    "    warmup_ratio=0.1,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=6,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_accuracy',\n",
    "    greater_is_better=True,\n",
    "    dataloader_num_workers=8,\n",
    "    dataloader_pin_memory=True,\n",
    "    save_safetensors=False,\n",
    "    report_to='wandb',\n",
    "    run_name='xlm-roberta-large',\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "try:\n",
    "    trainer.train()\n",
    "except Exception as e:\n",
    "    raise e\n",
    "finally:\n",
    "    wandb.finish()\n",
    "# Save the model\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a25349d7-2849-439c-ac74-8f750d87d76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Отзывы</th>\n",
       "      <th>разметка</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Очень разочарован QPhone. Ожидал, что квантовы...</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Очень быстро разряжается. Просто полное разоча...</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ну что тут сказать, телефон хороший, флагман т...</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QPhone Pro MAX просто бомба! Быстрее любого см...</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Слишком дорого для массового рынка – не каждый...</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Отзывы разметка\n",
       "0  Очень разочарован QPhone. Ожидал, что квантовы...        -\n",
       "1  Очень быстро разряжается. Просто полное разоча...        -\n",
       "2  Ну что тут сказать, телефон хороший, флагман т...        +\n",
       "3  QPhone Pro MAX просто бомба! Быстрее любого см...        +\n",
       "4  Слишком дорого для массового рынка – не каждый...        -"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the test data from CSV file\n",
    "test_df = pd.read_csv('test50.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e015e70-2d90-498d-804c-6573848cf491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "test_texts = test_df['Отзывы'].tolist()\n",
    "test_labels = (test_df['разметка'] == '+').astype(int).tolist()\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = SentimentDataset(test_texts, test_labels, tokenizer)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bc8f5564-27b8-4e6c-a65f-3e630d27588d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the best model from selected checkpoint\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "    'FacebookAI/xlm-roberta-large', num_labels=2,\n",
    ")\n",
    "# Configure LoRA parameters\n",
    "lora_config = QuantumLoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "model = apply_quantum_lora_to_xlm_roberta_model(model, lora_config)\n",
    "model.load_state_dict(torch.load('final-results/pytorch_model.bin', weights_only=True))\n",
    "model.to(device)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ba1b4869-8472-4984-a5dc-c989b023832f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results:\n",
      "Test Accuracy: 0.8400\n",
      "Test Loss: 0.8472\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.74      0.83        27\n",
      "           1       0.76      0.96      0.85        23\n",
      "\n",
      "    accuracy                           0.84        50\n",
      "   macro avg       0.86      0.85      0.84        50\n",
      "weighted avg       0.86      0.84      0.84        50\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[20  7]\n",
      " [ 1 22]]\n"
     ]
    }
   ],
   "source": [
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize metrics\n",
    "total_loss = 0\n",
    "predictions = []\n",
    "all_labels = []\n",
    "\n",
    "# Evaluation loop\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        # Move batch to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device) \n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Accumulate loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Get predictions\n",
    "        batch_preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        predictions.extend(batch_preds)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "avg_loss = total_loss / len(test_dataloader)\n",
    "accuracy = (np.array(predictions) == np.array(all_labels)).mean()\n",
    "\n",
    "print(\"\\nTest Results:\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(all_labels, predictions))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(all_labels, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
